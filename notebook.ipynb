{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Load SNP data into a NumPy array\n",
    "data = pd.read_csv('Data/S1_freq.csv.gz', sep='\\t', header=None, compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spherical or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "def is_data_spherical(X):\n",
    "    # Compute the centroid of the data\n",
    "    centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Calculate the Euclidean distances from each point to the centroid\n",
    "    distances = euclidean_distances(X, [centroid]).ravel()\n",
    "\n",
    "    # Compute the mean and standard deviation of the distances\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "\n",
    "    # Determine if the data is spherical\n",
    "    if std_distance / mean_distance < 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(is_data_spherical(data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# from column 1 to the end\n",
    "data2= scaler.fit_transform(data.iloc[:, 1:])\n",
    "\n",
    "# Choose the number of clusters\n",
    "k = 35\n",
    "# Initialize K centroids randomly\n",
    "kmeans = KMeans(n_clusters=k, init='random', n_init=10)\n",
    "\n",
    "# Fit the K-means model to the data\n",
    "kmeans.fit(data2)\n",
    "\n",
    "# Get the cluster assignments for each SNP\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centers\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Get the sum of squared distances of samples to their closest cluster center\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "# from first column of data get the country names and store in a list\n",
    "country = data.iloc[:, 0].to_list()\n",
    "\n",
    "# create a dictionary with country as key and labels as value\n",
    "country_labels = dict(zip(country, labels))\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print('Cluster labels: %s' % country_labels )\n",
    "print('Cluster centroids: %s' % centroids)\n",
    "print('Sum of squared distances: %s' % inertia)\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data2[:, 0], data2[:, 1], c=labels, cmap='rainbow')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(columns=['country', 'date', 'cluster'])\n",
    "\n",
    "for key, value in country_labels.items():\n",
    "    a= key.split('_')\n",
    "    b= a[0]\n",
    "    c= a[1]\n",
    "    d= value\n",
    "    e= f'{b}\\t{c}\\t{d}'\n",
    "    ## add to a dataframe\n",
    "    dataframe = dataframe.append({'country': b, 'date': c, 'cluster': d}, ignore_index=True)\n",
    "    \n",
    "# split country name if find capital letter in the middle of the string\n",
    "# and add a space before the capital letter\n",
    "dataframe['country'] = dataframe['country'].str.replace(r'(\\w)([A-Z])', r'\\1 \\2')\n",
    "dataframe.to_csv('Data/S1_label.csv', sep='\\t', index=False)   \n",
    "import pandas as pd\n",
    "\n",
    "newdf= pd.read_csv('Data/S1_label.csv', sep='\\t')\n",
    "newdf=pd.DataFrame(newdf)\n",
    "newdf['cluster'].value_counts()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = pd.DataFrame(df3)\n",
    "# get standard deviation of each column\n",
    "\n",
    "# calculate the mean of each column\n",
    "col_means = dfA.mean()\n",
    "\n",
    "# subtract each column mean from its values\n",
    "dfB = dfA.sub(col_means, axis=1)\n",
    "\n",
    "# create a new DataFrame with the resulting values\n",
    "dfB = pd.DataFrame(dfB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=data.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read in your dataset as a pandas dataframe\n",
    "\n",
    "\n",
    "# Apply UMAP to the dataset\n",
    "reducer = umap.UMAP()\n",
    "df_umap = reducer.fit_transform(data3)\n",
    "\n",
    "# Visualize the UMAP results\n",
    "plt.scatter(df_umap[:, 0], df_umap[:, 1], alpha=0.5)\n",
    "# color the points by their cluster assignment\n",
    "plt.scatter(df_umap[:, 0], df_umap[:, 1], c=labels, cmap='rainbow')\n",
    "\n",
    "\n",
    "\n",
    "# add a annotation very small font size and close to the point\n",
    "for i, txt in enumerate(country):\n",
    "    plt.annotate(txt, (df_umap[i, 0], df_umap[i, 1]), fontsize=6, xytext=(5, 2), textcoords='offset points')\n",
    "\n",
    "\n",
    "\n",
    "# add a title\n",
    "plt.title('UMAP projection of the SNP dataset')\n",
    "# add x and y labels\n",
    "plt.xlabel('UMAP1')\n",
    "plt.ylabel('UMAP2')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_silhouette_score(X, k_min=1, k_max=33):\n",
    "    # Create a range of K values\n",
    "    k_range = range(k_min, k_max+1)\n",
    "\n",
    "    # Create an empty list to store the silhouette scores\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Loop through the range of K values and calculate the silhouette score for each value\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot the silhouette scores for each K value\n",
    "    plt.plot(k_range, silhouette_scores)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Silhouette score for K-means clustering')\n",
    "    plt.xticks(np.arange(k_min, k_max+1, step=2))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_silhouette_score(data3, k_min=2, k_max=33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Load your dataset (assuming it's a CSV file)\n",
    "data = pd.read_csv('Data/DataS1_freq_labels.csv', sep='\\t')\n",
    "\n",
    "# Initialize a geolocator object\n",
    "geolocator = Nominatim(user_agent='my-app')\n",
    "\n",
    "# Define a function to get the latitude and longitude of a country\n",
    "def get_coordinates(country):\n",
    "    location = geolocator.geocode(country)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Apply the function to the 'country' column and create a new DataFrame\n",
    "coordinates = data['country'].apply(get_coordinates).apply(pd.Series)\n",
    "coordinates.columns = ['latitude', 'longitude']\n",
    "df = pd.concat([data, coordinates], axis=1)\n",
    "\n",
    "# Print the first few rows of the new DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Data/DataS1_freq_labels (curated).csv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(data3)\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 16 # Number of clusters\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, eigen_solver='arpack', affinity=\"nearest_neighbors\").fit(X)\n",
    "\n",
    "# Visualize the clustering result\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clustering.labels_)\n",
    "plt.title(\"Spectral Clustering ({} clusters)\".format(n_clusters))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset into a Pandas dataframe or NumPy array\n",
    "X = data3\n",
    "\n",
    "# Normalize or standardize your dataset, if needed\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Choose the clustering algorithms you want to use\n",
    "clustering_algorithms = [\n",
    "    KMeans(n_clusters=2),\n",
    "    DBSCAN(),\n",
    "    AgglomerativeClustering(n_clusters=2),\n",
    " \n",
    "]\n",
    "\n",
    "# Set up a dictionary to store the results and evaluation metrics for each algorithm\n",
    "results = {}\n",
    "\n",
    "# Iterate over each clustering algorithm and evaluate its performance\n",
    "for algorithm in clustering_algorithms:\n",
    "    # Fit the model to your data and predict cluster labels\n",
    "    algorithm.fit(X)\n",
    "    labels = algorithm.labels_\n",
    "    \n",
    "    # Compute one or more clustering evaluation metrics\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    ch_index = calinski_harabasz_score(X, labels)\n",
    "    db_index = davies_bouldin_score(X, labels)\n",
    "    \n",
    "    # Store the results and evaluation metrics for this algorithm in the dictionary\n",
    "    results[str(algorithm)] = {'labels': labels, 'silhouette': silhouette, 'ch_index': ch_index, 'db_index': db_index}\n",
    "    \n",
    "# Print the results for each algorithm\n",
    "for algorithm, result in results.items():\n",
    "    print(algorithm)\n",
    "    print('Labels:', result['labels'])\n",
    "    print('Silhouette:', result['silhouette'])\n",
    "    print('Calinski-Harabasz Index:', result['ch_index'])\n",
    "    print('Davies-Bouldin Index:', result['db_index'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
