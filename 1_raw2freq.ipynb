{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bim File\n",
    "\n",
    "variant identifier and minor allele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_bim=pd.read_csv('Data/0_Raw/DataS1.bim', sep='\\t', header=None)\n",
    "# df_bim = 1 and 4 columns\n",
    "df_bim = df_bim[[1, 4]]\n",
    "\n",
    "df_bim.rename(columns={1: 'VariantID', 4: 'minor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver df_bim['variantID'] to a list\n",
    "columns= df_bim['VariantID'].to_list()\n",
    "columns.insert(0, 'familyID')\n",
    "columns.insert(1, 'individualID')\n",
    "columns.insert(2, 'fatherID')\n",
    "columns.insert(3, 'motherID')\n",
    "columns.insert(4,'Sex')\n",
    "columns.insert(5, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor=df_bim['minor'].to_list()\n",
    "minor.insert(0, 'None')\n",
    "minor.insert(1, 'None')\n",
    "minor.insert(2, 'None')\n",
    "minor.insert(3, 'None')\n",
    "minor.insert(4,'None')\n",
    "minor.insert(5, 'None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ped File\n",
    "\n",
    "familly id and individual id and allels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the ped file by familly id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r Data/1_raw2freq/countries/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Norway', 'Croatia', 'CzechRepublic', 'Germany', 'Romania', 'Sweden', 'Turkey', 'Armenia', 'Kazakhstan', 'Switzerland', 'Estonia', 'TheNetherlands', 'Denmark', 'Greece', 'Poland', 'Jordan', 'Spain', 'Luxembourg', 'Lithuania', 'UnitedKingdom', 'Austria', 'Georgia', 'Macedonia', 'Bulgaria', 'Russia', 'Israel', 'Latvia', 'Iran', 'Italy', 'Portugal', 'France', 'Serbia', 'Ukraine', 'Hungary']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "def split_and_convert(file_path):\n",
    "    # Create a directory to store output files\n",
    "    os.makedirs('Data/1_raw2freq/countries', exist_ok=True)\n",
    "    populations = []\n",
    "    with open(file_path, 'r') as ped_file:\n",
    "        for row in ped_file:\n",
    "            row_data = row.strip().split()\n",
    "            populations.append(row_data[0])\n",
    "            pedigree = row_data[0]\n",
    "            with open(f'Data/1_raw2freq/countries/{pedigree}.txt', 'a') as out_file:\n",
    "                # Join the pairs of alleles in each row\n",
    "                new_fields = row_data[:6]\n",
    "                for i in range(6, len(row_data), 2):\n",
    "                    new_fields.append(row_data[i] + row_data[i+1])\n",
    "                # Write the new fields to the output file\n",
    "                out_file.write(' '.join(new_fields) + '\\n')\n",
    "        \n",
    "    return list(set(populations))\n",
    "\n",
    "file_path = 'Data/0_Raw/DataS1.ped'\n",
    "\n",
    "populations = split_and_convert(file_path)\n",
    "print(populations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeuro = pd.read_excel(\n",
    "    'Data/0_Raw/Eurasian - Dataset_tims.xlsx', sheet_name='Eurasian')\n",
    "dfeuro = pd.DataFrame(dfeuro)\n",
    "dfeuro.rename(columns={\n",
    "              'Date mean in BP in years before 1950 CE [OxCal mu for a direct radiocarbon date, and average of range for a contextual date]': 'date'}, inplace=True)\n",
    "\n",
    "# date_range is a list of the range of dates from 0 to 12000 years ago in 1000 year intervals\n",
    "date_range = list(range(0, 12000, 1000)) \n",
    "# if dfeuro['date'] is in the range of date_range, then dfeuro['cat_date'] is the index of the range\n",
    "dfeuro['cat_date']=pd.cut(dfeuro['date'], date_range, labels=range(0, len(date_range)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq(pop_path, country):\n",
    "    '''This function takes a population file and returns a dictionary of the frequency of each variant'''\n",
    "    df = pd.read_csv(pop_path, sep=' ', header=None)\n",
    "    df.columns = columns\n",
    "    df.date = 0\n",
    "    # print(len(df))\n",
    "    # locate df['individualID'] in dfeuro['Master ID'] and add df['cat_date'] to the df[date]\n",
    "    for i in range(0, len(df)):\n",
    "        for j in range(0, len(dfeuro)):\n",
    "            if df['individualID'][i] == dfeuro['Master ID'][j]:\n",
    "                df['date'][i] = dfeuro['cat_date'][j]\n",
    "  \n",
    "    # get unique dates in df['date'] and store in a list and remove None\n",
    "    date = df['date'].unique()\n",
    "    date = date.tolist()\n",
    "    if 0 in date:\n",
    "        date.remove(0)\n",
    "    \n",
    "\n",
    "    countryfreq = {}\n",
    "    \n",
    "    for d in date:\n",
    "        freqdate= []\n",
    "        df2 = df[df['date'] == d]\n",
    "        for i in range(6, len(df.columns)):\n",
    "            minorAllele = minor[i]\n",
    "            count = 0\n",
    "            total = len(df2)*2\n",
    "            for j in range(0, len(df2)):\n",
    "                # count occurrences of the minor allele in column i\n",
    "                count += str(df2.iloc[j, i]).count(str(minorAllele))\n",
    "            freqdate.append(count/total)\n",
    "            \n",
    "        name= f'{country}_{d}'\n",
    "        countryfreq[name] = freqdate\n",
    "        \n",
    "    return countryfreq\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "! rm Data/S1_freq.csv\n",
    "def process_country(country):\n",
    "    pop_path = f'Data/1_raw2freq/countries/{country}.txt'\n",
    "    freq_results = freq(pop_path, country)\n",
    "    with open('Data/1_raw2freq/S1_freq.csv', 'a') as f:\n",
    "        for key, value in freq_results.items():\n",
    "            str_values = [str(val) for val in value]  # convert float values to strings\n",
    "            f.write('%s\\t%s\\n' % (key, '\\t'.join(str_values)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.map(process_country, populations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter SNPs which have 0 in more than 80% of the countries-age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# open S1_freq.csv.gz and make a df and calculate the mean of each column\n",
    "df = pd.read_csv('Data/1_raw2freq/S1_freq.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=[]\n",
    "for i in range(0, len(df.columns)):\n",
    "   # count the number of times a value == 0 in each column\n",
    "    count = df[i].value_counts().get(0)\n",
    "    dff.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 1058)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.copy()\n",
    "# get indeces of dff where that are > 80\n",
    "index = [i for i, x in enumerate(dff) if x > 95]\n",
    "# remove df2 columns that are in index\n",
    "df2.drop(df2.columns[index], axis=1, inplace=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv without index and header\n",
    "# df2.to_csv('Data/1_raw2freq/S1_freq_filtered.csv', index=False, header=False, sep='\\t')\n",
    "\n",
    "df2.to_csv('Data/1_raw2freq/test.csv', index=False, header=False, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert every *.csv file in the directory to *.csv.gz\n",
    "! gzip -9 Data/1_raw2freq/*.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('Data/1_raw2freq/test.csv.gz', sep='\\t', header=None, compression='gzip')\n",
    "\n",
    "# convert first column to a list\n",
    "pop = df[0].tolist()\n",
    "cont=[]\n",
    "date=[]\n",
    "for i in range(0, len(pop)):\n",
    "    # split the first column at '_' and store the first element in a list\n",
    "    x = pop[i].split('_')\n",
    "    cont.append(x[0])\n",
    "    date.append((int(x[1]))*1000)\n",
    "\n",
    "# add cont and date to df as columns 1 and 2\n",
    "df.insert(1, 'country', cont)\n",
    "df.insert(2, 'date', date)\n",
    "\n",
    "# drop the first column\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "columns= ['country']+['date'] + [f'SNP{str(i)}' for i in range(1, len(df.columns)-1)]\n",
    "df.columns = columns\n",
    "df\n",
    "df.to_csv('Data/1_raw2freq/test_1.csv', sep='\\t')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>SNP1</th>\n",
       "      <th>SNP2</th>\n",
       "      <th>SNP3</th>\n",
       "      <th>SNP4</th>\n",
       "      <th>SNP5</th>\n",
       "      <th>SNP6</th>\n",
       "      <th>SNP7</th>\n",
       "      <th>SNP8</th>\n",
       "      <th>...</th>\n",
       "      <th>SNP1048</th>\n",
       "      <th>SNP1049</th>\n",
       "      <th>SNP1050</th>\n",
       "      <th>SNP1051</th>\n",
       "      <th>SNP1052</th>\n",
       "      <th>SNP1053</th>\n",
       "      <th>SNP1054</th>\n",
       "      <th>SNP1055</th>\n",
       "      <th>SNP1056</th>\n",
       "      <th>SNP1057</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Denmark</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>6000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Estonia</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Hungary</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>UnitedKingdom</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>UnitedKingdom</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.419355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>UnitedKingdom</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.242424</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.393939</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>UnitedKingdom</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119 rows Ã— 1059 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  date      SNP1      SNP2      SNP3      SNP4      SNP5  \\\n",
       "0          Denmark  3000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1          Denmark  4000  0.000000  0.000000  0.500000  0.000000  0.500000   \n",
       "2          Denmark  2000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3          Estonia  6000  0.500000  1.000000  0.500000  0.500000  0.500000   \n",
       "4          Estonia  5000  1.000000  0.000000  1.000000  0.000000  0.000000   \n",
       "..             ...   ...       ...       ...       ...       ...       ...   \n",
       "114        Hungary  2000  0.000000  1.000000  1.000000  1.000000  0.000000   \n",
       "115  UnitedKingdom  3000  0.326087  0.565217  0.500000  0.434783  0.347826   \n",
       "116  UnitedKingdom  5000  0.193548  0.290323  0.419355  0.419355  0.258065   \n",
       "117  UnitedKingdom  4000  0.333333  0.424242  0.393939  0.333333  0.242424   \n",
       "118  UnitedKingdom  2000  0.500000  0.333333  0.000000  0.333333  0.000000   \n",
       "\n",
       "         SNP6      SNP7      SNP8  ...   SNP1048   SNP1049   SNP1050  \\\n",
       "0    0.000000  1.000000  1.000000  ...  0.000000  1.000000  0.000000   \n",
       "1    0.000000  0.500000  0.500000  ...  0.500000  1.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.500000  0.500000  0.500000  ...  0.000000  0.500000  0.000000   \n",
       "4    0.500000  0.000000  0.500000  ...  0.000000  0.500000  0.500000   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "114  0.000000  0.000000  1.000000  ...  1.000000  1.000000  1.000000   \n",
       "115  0.434783  0.413043  0.282609  ...  0.434783  0.369565  0.304348   \n",
       "116  0.451613  0.419355  0.580645  ...  0.193548  0.290323  0.451613   \n",
       "117  0.333333  0.272727  0.303030  ...  0.272727  0.424242  0.393939   \n",
       "118  0.666667  0.333333  0.500000  ...  0.500000  0.333333  0.666667   \n",
       "\n",
       "      SNP1051   SNP1052   SNP1053   SNP1054   SNP1055   SNP1056   SNP1057  \n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.500000  0.500000  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  1.000000  0.000000  \n",
       "3    0.500000  0.500000  0.000000  0.500000  1.000000  0.500000  0.000000  \n",
       "4    0.500000  0.500000  0.500000  0.000000  0.000000  0.500000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "114  1.000000  0.000000  0.000000  0.000000  1.000000  1.000000  1.000000  \n",
       "115  0.347826  0.543478  0.413043  0.326087  0.391304  0.456522  0.500000  \n",
       "116  0.483871  0.483871  0.387097  0.483871  0.548387  0.354839  0.419355  \n",
       "117  0.333333  0.454545  0.363636  0.484848  0.303030  0.363636  0.363636  \n",
       "118  0.166667  0.500000  0.333333  0.000000  0.500000  0.500000  0.333333  \n",
       "\n",
       "[119 rows x 1059 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
