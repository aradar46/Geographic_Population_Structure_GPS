{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bim File\n",
    "\n",
    "variant identifier and minor allele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_bim=pd.read_csv('Data/0_Raw/DataS1.bim', sep='\\t', header=None)\n",
    "# df_bim = 1 and 4 columns\n",
    "df_bim = df_bim[[1, 4]]\n",
    "\n",
    "df_bim.rename(columns={1: 'VariantID', 4: 'minor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver df_bim['variantID'] to a list\n",
    "columns= df_bim['VariantID'].to_list()\n",
    "columns.insert(0, 'familyID')\n",
    "columns.insert(1, 'individualID')\n",
    "columns.insert(2, 'fatherID')\n",
    "columns.insert(3, 'motherID')\n",
    "columns.insert(4,'Sex')\n",
    "columns.insert(5, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minor=df_bim['minor'].to_list()\n",
    "minor.insert(0, 'None')\n",
    "minor.insert(1, 'None')\n",
    "minor.insert(2, 'None')\n",
    "minor.insert(3, 'None')\n",
    "minor.insert(4,'None')\n",
    "minor.insert(5, 'None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ped File\n",
    "\n",
    "familly id and individual id and allels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the ped file by familly id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r Data/1_raw2freq/countries/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Norway', 'Croatia', 'CzechRepublic', 'Germany', 'Romania', 'Sweden', 'Turkey', 'Armenia', 'Kazakhstan', 'Switzerland', 'Estonia', 'TheNetherlands', 'Denmark', 'Greece', 'Poland', 'Jordan', 'Spain', 'Luxembourg', 'Lithuania', 'UnitedKingdom', 'Austria', 'Georgia', 'Macedonia', 'Bulgaria', 'Russia', 'Israel', 'Latvia', 'Iran', 'Italy', 'Portugal', 'France', 'Serbia', 'Ukraine', 'Hungary']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    " \n",
    "def split_and_convert(file_path):\n",
    "    # Create a directory to store output files\n",
    "    os.makedirs('Data/1_raw2freq/countries', exist_ok=True)\n",
    "    populations = []\n",
    "    with open(file_path, 'r') as ped_file:\n",
    "        for row in ped_file:\n",
    "            row_data = row.strip().split()\n",
    "            populations.append(row_data[0])\n",
    "            pedigree = row_data[0]\n",
    "            with open(f'Data/1_raw2freq/countries/{pedigree}.txt', 'a') as out_file:\n",
    "                # Join the pairs of alleles in each row\n",
    "                new_fields = row_data[:6]\n",
    "                for i in range(6, len(row_data), 2):\n",
    "                    new_fields.append(row_data[i] + row_data[i+1])\n",
    "                # Write the new fields to the output file\n",
    "                out_file.write(' '.join(new_fields) + '\\n')\n",
    "        \n",
    "    return list(set(populations))\n",
    "\n",
    "file_path = 'Data/0_Raw/DataS1.ped'\n",
    "\n",
    "populations = split_and_convert(file_path)\n",
    "print(populations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfeuro = pd.read_excel(\n",
    "    'Data/0_Raw/Eurasian - Dataset_tims.xlsx', sheet_name='Eurasian')\n",
    "dfeuro = pd.DataFrame(dfeuro)\n",
    "dfeuro.rename(columns={\n",
    "              'Date mean in BP in years before 1950 CE [OxCal mu for a direct radiocarbon date, and average of range for a contextual date]': 'date'}, inplace=True)\n",
    "\n",
    "# date_range is a list of the range of dates from 0 to 12000 years ago in 1000 year intervals\n",
    "date_range = list(range(0, 12000, 1000)) \n",
    "# if dfeuro['date'] is in the range of date_range, then dfeuro['cat_date'] is the index of the range\n",
    "dfeuro['cat_date']=pd.cut(dfeuro['date'], date_range, labels=range(0, len(date_range)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def freq(pop_path, country):\n",
    "    '''This function takes a population file and returns a dictionary of the frequency of each variant'''\n",
    "    df = pd.read_csv(pop_path, sep=' ', header=None)\n",
    "    df.columns = columns\n",
    "    df.date = 0\n",
    "    # print(len(df))\n",
    "    # locate df['individualID'] in dfeuro['Master ID'] and add df['cat_date'] to the df[date]\n",
    "    for i in range(0, len(df)):\n",
    "        for j in range(0, len(dfeuro)):\n",
    "            if df['individualID'][i] == dfeuro['Master ID'][j]:\n",
    "                df['date'][i] = dfeuro['cat_date'][j]\n",
    "  \n",
    "    # get unique dates in df['date'] and store in a list and remove None\n",
    "    date = df['date'].unique()\n",
    "    date = date.tolist()\n",
    "    if 0 in date:\n",
    "        date.remove(0)\n",
    "    \n",
    "\n",
    "    countryfreq = {}\n",
    "    \n",
    "    for d in date:\n",
    "        freqdate= []\n",
    "        df2 = df[df['date'] == d]\n",
    "        for i in range(6, len(df.columns)):\n",
    "            minorAllele = minor[i]\n",
    "            count = 0\n",
    "            total = len(df2)*2\n",
    "            for j in range(0, len(df2)):\n",
    "                # count occurrences of the minor allele in column i\n",
    "                count += str(df2.iloc[j, i]).count(str(minorAllele))\n",
    "            freqdate.append(count/total)\n",
    "            \n",
    "        name= f'{country}_{d}'\n",
    "        countryfreq[name] = freqdate\n",
    "        \n",
    "    return countryfreq\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "! rm Data/S1_freq.csv\n",
    "def process_country(country):\n",
    "    pop_path = f'Data/1_raw2freq/countries/{country}.txt'\n",
    "    freq_results = freq(pop_path, country)\n",
    "    with open('Data/1_raw2freq/S1_freq.csv', 'a') as f:\n",
    "        for key, value in freq_results.items():\n",
    "            str_values = [str(val) for val in value]  # convert float values to strings\n",
    "            f.write('%s\\t%s\\n' % (key, '\\t'.join(str_values)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        pool.map(process_country, populations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter SNPs which have 0 in more than 80% of the countries-age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open S1_freq.csv.gz and make a df and calculate the mean of each column\n",
    "df = pd.read_csv('Data/1_raw2freq/S1_freq.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff=[]\n",
    "for i in range(0, len(df.columns)):\n",
    "   # count the number of times a value == 0 in each column\n",
    "    count = df[i].value_counts().get(0)\n",
    "    dff.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 86686)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.copy()\n",
    "# get indeces of dff where that are > 80\n",
    "index = [i for i, x in enumerate(dff) if x > 95]\n",
    "# remove df2 columns that are in index\n",
    "df2.drop(df2.columns[index], axis=1, inplace=True)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv without index and header\n",
    "df2.to_csv('Data/1_raw2freq/S1_freq_filtered.csv', index=False, header=False, sep='\\t')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = pd.DataFrame(df2.iloc[:, 1:])\n",
    "# get standard deviation of each column\n",
    "\n",
    "# calculate the mean of each column\n",
    "col_means = dfA.mean()\n",
    "\n",
    "# subtract each column mean from its values\n",
    "dfB = dfA.sub(col_means, axis=1)\n",
    "\n",
    "# create a new DataFrame with the resulting values\n",
    "dfB = pd.DataFrame(dfB)\n",
    "\n",
    "dfB.to_csv('Data/1_raw2freq/S1_freq_filtered_SD.csv',\n",
    "           index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfC= pd.DataFrame(df.iloc[:, 1:])\n",
    "col_means = dfC.mean(axis=0)\n",
    "dfD= dfC.sub(col_means, axis=1)\n",
    "dfD.to_csv('Data/1_raw2freq/S1_freq_SD.csv', index=False, header=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert every *.csv file in the directory to *.csv.gz\n",
    "! gzip -9 Data/1_raw2freq/*.csv\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
