{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('Data/S1_freq.csv.gz', sep='\\t', header=None, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data.iloc[:, 1:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# convert to float\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df2 \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m df3 \u001b[39m=\u001b[39m df2\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39m, \u001b[39m17520251\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# remove 251 columns from the end\u001b[39;00m\n\u001b[1;32m     12\u001b[0m df3 \u001b[39m=\u001b[39m df3[:, :\u001b[39m-\u001b[39m\u001b[39m251\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/pli/lib/python3.9/site-packages/pandas/core/generic.py:5465\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5463\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name):\n\u001b[1;32m   5464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5465\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# convert to numpy array\n",
    "df2 = df.values\n",
    "\n",
    "# convert to float\n",
    "df2 = df.astype('float32')\n",
    " \n",
    "\n",
    "\n",
    "df3 = df2.reshape(1, 17520251)\n",
    "\n",
    "# remove 251 columns from the end\n",
    "df3 = df3[:, :-251]\n",
    "\n",
    "df3= df3.reshape(1000, 17520)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfA = pd.DataFrame(df3)\n",
    "# get standard deviation of each column\n",
    "\n",
    "# calculate the mean of each column\n",
    "col_means = dfA.mean()\n",
    "\n",
    "# subtract each column mean from its values\n",
    "dfB = dfA.sub(col_means, axis=1)\n",
    "\n",
    "# create a new DataFrame with the resulting values\n",
    "dfB = pd.DataFrame(dfB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# create a dataframe\n",
    "df4 = pd.DataFrame(df3)\n",
    "countries = ['Hungary', 'Croatia', 'Jordan', 'France',\n",
    "             'Kazakhstan', 'Iran', 'Denmark', 'Luxembourg',\n",
    "             'Estonia', 'CzechRepublic', 'Italy', 'Sweden', \n",
    "             'United Kingdom', 'Armenia', 'Germany', 'Spain',\n",
    "             'Switzerland', 'Latvia', 'Romania', 'The Netherlands', \n",
    "             'Turkey', 'Russia', 'Ukraine', 'Bulgaria', 'Poland', \n",
    "             'Austria', 'Georgia', 'Portugal', 'Israel',\n",
    "             'Lithuania', 'Macedonia', 'Serbia', 'Greece', 'Norway']\n",
    "\n",
    "age = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000]\n",
    "\n",
    "# create 1000 random combinations of country_age\n",
    "country= []\n",
    "bp = []\n",
    "for i in range(1000):\n",
    "    country.append(random.choice(countries))\n",
    "    bp.append(random.choice(age))\n",
    "    \n",
    "# add country and age to the beginning of the dataframe\n",
    "df4.insert(0, 'country', country)\n",
    "df4.insert(1, 'age', bp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names to SNP 1-17520\n",
    "df4.columns = ['country']+['age'] + ['SNP' + str(i) for i in range(1, 17521)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "def is_data_spherical(X):\n",
    "    # Compute the centroid of the data\n",
    "    centroid = np.mean(X, axis=0)\n",
    "\n",
    "    # Calculate the Euclidean distances from each point to the centroid\n",
    "    distances = euclidean_distances(X, [centroid]).ravel()\n",
    "\n",
    "    # Compute the mean and standard deviation of the distances\n",
    "    mean_distance = np.mean(distances)\n",
    "    std_distance = np.std(distances)\n",
    "\n",
    "    # Determine if the data is spherical\n",
    "    if std_distance / mean_distance < 0.5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(is_data_spherical(dfB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_silhouette_score(X, k_min=1, k_max=33):\n",
    "    # Create a range of K values\n",
    "    k_range = range(k_min, k_max+1)\n",
    "\n",
    "    # Create an empty list to store the silhouette scores\n",
    "    silhouette_scores = []\n",
    "\n",
    "    # Loop through the range of K values and calculate the silhouette score for each value\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        silhouette_avg = silhouette_score(X, labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "    # Plot the silhouette scores for each K value\n",
    "    plt.plot(k_range, silhouette_scores)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Silhouette score')\n",
    "    plt.title('Silhouette score for K-means clustering')\n",
    "    plt.xticks(np.arange(k_min, k_max+1, step=2))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "plot_silhouette_score(dfB, k_min=2, k_max=33)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def calculate_inertia(X, k):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    return kmeans.inertia_\n",
    "\n",
    "\n",
    "def plot_elbow(X, k_min=1, k_max=33, n_jobs=-1):\n",
    "    # Create a range of K values\n",
    "    k_range = range(k_min, k_max+1)\n",
    "\n",
    "    # Calculate inertias for each value of K in parallel\n",
    "    inertias = Parallel(n_jobs=n_jobs)(delayed(calculate_inertia)(X, k) for k in k_range)\n",
    "\n",
    "    # Plot the inertias for each K value\n",
    "    plt.plot(k_range, inertias)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.title('Inertia for K-means clustering')\n",
    "    plt.xticks(np.arange(k_min, k_max+1, step=2))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_elbow(dfB, k_min=2, k_max=33, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# from column 1 to the end\n",
    "data2= scaler.fit_transform(dfB)\n",
    "\n",
    "# Choose the number of clusters\n",
    "k = 24\n",
    "# Initialize K centroids randomly\n",
    "kmeans = KMeans(n_clusters=k, init='random', n_init=10)\n",
    "\n",
    "# Fit the K-means model to the data\n",
    "kmeans.fit(dfB)\n",
    "\n",
    "# Get the cluster assignments for each SNP\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Get the cluster centers\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Get the sum of squared distances of samples to their closest cluster center\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "# from first column of data get the country names and store in a list\n",
    "country = data.iloc[:, 0].to_list()\n",
    "\n",
    "# create a dictionary with country as key and labels as value\n",
    "country_labels = dict(zip(country, labels))\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print('Cluster labels: %s' % country_labels )\n",
    "print('Cluster centroids: %s' % centroids)\n",
    "print('Sum of squared distances: %s' % inertia)\n",
    "\n",
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(data2[:, 0], data2[:, 1], c=labels, cmap='rainbow')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(dfA)\n",
    "\n",
    "#\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 15 # Number of clusters\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, eigen_solver='arpack', affinity=\"nearest_neighbors\").fit(X)\n",
    "\n",
    "# evaluate the clustering result\n",
    "print(silhouette_score(X, clustering.labels_))\n",
    "\n",
    "\n",
    "# Visualize the clustering result\n",
    "plt.scatter(X[:, 0], X[:, 1], c=clustering.labels_)\n",
    "plt.title(\"Spectral Clustering ({} clusters)\".format(n_clusters))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pli",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
